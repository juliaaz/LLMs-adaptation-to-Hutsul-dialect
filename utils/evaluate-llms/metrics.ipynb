{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc7221a7-7b61-415e-bac3-f116b36ee397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d71233b6-5e28-4bc1-8d95-51a26b5d21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2773636d-e3d3-47e2-b125-fc49df2722d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc2d8c9-d071-4c15-88a5-bbae9097d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_and_concatenate_text_by_language(data):\n",
    "    clean_data = re.sub(r'\\d+|â€”', '', data)\n",
    "    pattern = re.compile(r\"(Ukrainian|Hutsul):(.*?)(?=(Ukrainian|Hutsul):|$)\", re.DOTALL)\n",
    "    text_by_language = {'Ukrainian': \"\", 'Hutsul': \"\"}\n",
    "    matches = pattern.findall(clean_data)\n",
    "    for lang, text, _ in matches:\n",
    "        text_by_language[lang] += text.strip() + \" \"\n",
    "\n",
    "    return {'Ukrainian': text_by_language['Ukrainian'].strip(), 'Hutsul': text_by_language['Hutsul'].strip()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cfff5b0-0e65-4644-944a-2bab848ac193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_scores(scores):\n",
    "    bleu_scores = [score[2] for score in scores]\n",
    "    labse_scores = [score[3] for score in scores]\n",
    "    \n",
    "    average_bleu = np.mean(bleu_scores)\n",
    "    median_bleu = np.median(bleu_scores)\n",
    "    average_labse = np.mean(labse_scores)\n",
    "    median_labse = np.median(labse_scores)\n",
    "    \n",
    "    #print(f\"Average BLEU Score: {average_bleu}\")\n",
    "    print(f\"Median BLEU Score: {median_bleu}\")\n",
    "    #print(f\"Average LaBSE Score: {average_labse}\")\n",
    "    print(f\"Median LaBSE Score: {median_labse}\")\n",
    "    \n",
    "    return average_bleu, average_labse#average_bleu, median_bleu, average_labse, median_labse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c997b54d-4a8a-4f7f-980f-f0dbc4d11405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_calculate_bleu(data):\n",
    "    j=0\n",
    "    segments = data.split('-------------------------------')\n",
    "    bleu_scores1 = []\n",
    "    bleu_scores2 = []\n",
    "    for segment in segments:\n",
    "        if segment.strip():\n",
    "            j+=1\n",
    "            \n",
    "            parts = segment.split('!!!')\n",
    "            concatenated_data = split_and_concatenate_text_by_language(parts[0])\n",
    "            ukrainian_text1 = concatenated_data['Ukrainian']\n",
    "            hutsul_text1 = concatenated_data['Hutsul']\n",
    "            reference1 = word_tokenize(ukrainian_text1)\n",
    "            candidate1 = word_tokenize(hutsul_text1)\n",
    "            \n",
    "            concatenated_data = split_and_concatenate_text_by_language(parts[1])\n",
    "            ukrainian_text2 = concatenated_data['Ukrainian']\n",
    "            hutsul_text2 = concatenated_data['Hutsul']\n",
    "            reference2 = word_tokenize(hutsul_text2)\n",
    "            candidate2 = word_tokenize(ukrainian_text2) \n",
    "\n",
    "            source_embedding = model.encode(hutsul_text2, convert_to_tensor=True)\n",
    "            translated_embedding = model.encode(hutsul_text1, convert_to_tensor=True)\n",
    "            cosine_similarity = util.pytorch_cos_sim(source_embedding, translated_embedding)\n",
    "            \n",
    "            score = sentence_bleu([reference2], candidate1)\n",
    "            bleu_scores1.append((hutsul_text2, hutsul_text1, score, cosine_similarity.item()))\n",
    "            \n",
    "            source_embedding = model.encode(ukrainian_text1, convert_to_tensor=True)\n",
    "            translated_embedding = model.encode(ukrainian_text2, convert_to_tensor=True)\n",
    "            cosine_similarity = util.pytorch_cos_sim(source_embedding, translated_embedding)\n",
    "\n",
    "            score = sentence_bleu([reference1], candidate2)\n",
    "            bleu_scores2.append((ukrainian_text1, ukrainian_text2, score, cosine_similarity.item()))\n",
    "    return bleu_scores1, bleu_scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b0f2065-13f5-4a4c-a55b-9f73ab3a3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data_from_file('llama-3-hutsul-finetune-combined_eval_post.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c988a7-5c92-4761-bd87-0bcf65e24b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, scores2 = parse_and_calculate_bleu(data)\n",
    "for original, translation, score, cosine_similarity in scores:\n",
    "    print(f\"Original: {original}\\nTranslation: {translation}\\nBLEU Score: {score}\\nLaBSE Score: {cosine_similarity}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123cbb2-e4cf-4fb9-b203-95a53541a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for original, translation, score, cosine_similarity in scores2:\n",
    "    print(f\"Original: {original}\\nTranslation: {translation}\\nBLEU Score: {score}\\nLaBSE Score: {cosine_similarity}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6840ad3a-db87-4c0e-9838-a0d6dbd40bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median BLEU Score: 2.66669249540442e-78\n",
      "Median LaBSE Score: 0.7877578735351562\n",
      "(0.1713403423750469, 0.6732441560462391)\n",
      "---------------------\n",
      "Median BLEU Score: 0.24384183193426084\n",
      "Median LaBSE Score: 0.882727861404419\n",
      "(0.27870532035242085, 0.8437750962065773)\n"
     ]
    }
   ],
   "source": [
    "print(get_avg_scores(scores))\n",
    "print('---------------------')\n",
    "print(get_avg_scores(scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c88b3a31-68a8-41a7-9912-07ea86bf5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data_from_file('m_eval_done.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "945d82d9-0b39-4bc0-a75e-3d1d3df9bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_scores, m_scores2 = parse_and_calculate_bleu(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ee0266a-66ad-4c97-9df2-bd807002f31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median BLEU Score: 4.0622028886850106e-78\n",
      "Median LaBSE Score: 0.7612000703811646\n",
      "(0.15998786146580393, 0.7133132157723109)\n",
      "---------------------\n",
      "Median BLEU Score: 0.17376436413676968\n",
      "Median LaBSE Score: 0.8063327074050903\n",
      "(0.21484042900157818, 0.7642296598354975)\n"
     ]
    }
   ],
   "source": [
    "print(get_avg_scores(m_scores))\n",
    "print('---------------------')\n",
    "print(get_avg_scores(m_scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e87ec2a5-7c2e-43a6-ae22-4a8f87017fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data_from_file('chatGpt-3.5_preprocessed_eval.txt')\n",
    "gpt_scores, gpt_scores2 = parse_and_calculate_bleu(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4be4fccc-400a-4e42-b67f-a566118195cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median BLEU Score: 4.2077067846993234e-78\n",
      "Median LaBSE Score: 0.8610229194164276\n",
      "(0.12099316710994074, 0.8314545747637748)\n",
      "---------------------\n",
      "Median BLEU Score: 0.28582555866764475\n",
      "Median LaBSE Score: 0.932696521282196\n",
      "(0.3095447983505376, 0.9078365230560302)\n"
     ]
    }
   ],
   "source": [
    "print(get_avg_scores(gpt_scores))\n",
    "print('---------------------')\n",
    "print(get_avg_scores(gpt_scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d514f019-0c26-4d6f-a4d5-e27459dd11ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median BLEU Score: 3.1537317208636084e-78\n",
      "Median LaBSE Score: 0.747868001461029\n",
      "(0.14063703910833475, 0.6724886310005945)\n",
      "---------------------\n",
      "Median BLEU Score: 0.2102369368326755\n",
      "Median LaBSE Score: 0.772828996181488\n",
      "(0.22987647790345245, 0.720658637701519)\n"
     ]
    }
   ],
   "source": [
    "data = read_data_from_file('mistral-hutsul-finetuned-v1_eval.txt')\n",
    "v1_scores, v1_scores2 = parse_and_calculate_bleu(data)\n",
    "\n",
    "print(get_avg_scores(v1_scores))\n",
    "print('---------------------')\n",
    "print(get_avg_scores(v1_scores2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
